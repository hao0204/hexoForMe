---
title: 神经网络的算法实现
date: 2017-12-24 15:38:29
tags: [机器学习]
categories: [技术]
---

### 内容简介

本文主要讲述三层神经网络的算法实现，代码实现较为简单，仅供学习使用。

<!-- more -->

### 神经网络的算法推导

看过众多的神经网络算法的推导，Github上的《BP 算法原理和详细推导流程.pdf》写的最好。参考资料中的第一条就是其链接。这里也提供了<a href="/images/神经网络的算法实现/BP 算法原理和详细推导流程.pdf">备份版</a>。

### 神经网络的算法实现

我把神经网络的算法实现分为三个部分：

1. 神经元
2. 激活函数
3. 神经网络

#### 神经元

按照所处的位置划分，神经元有三种，分别为输入层、隐藏层、输出层上的神经元。
即：
	
	enum NodeType {
		INPUT, HIDDEN, OUTPUT
	}

单个神经元上存储了前向传播时的输入和输出，反向传播时的输入和输出，以及所使用的激活函数。即：

	class NetworkNode {
		private NodeType nodeType; //神经元类型
		private double forwardInputValue; //前向传播时的输入
		private double forwardOutputValue; //前向传播时的输出
		private double backwardInputValue; //反向传播时的输入
		private double backwardOutputValue; //反向传播时的输出
		private ActivationFunction activationFunction; //激活函数
		...
	}
	
Getter 和 Setter函数这里不再细列。

#### 神经网络

神经网络存储了输入层、隐藏层、输出层的神经元信息以及权重。即：

	class AnnClassifier {
		private int inputNodeCount; //输入层神经元个数
		private int hiddenNodeCount; //隐藏层神经元个数
		private int outputNodeCount; //输出层神经元个数
		
		private List<NetworkNode> inputNodes; //输入层的所有神经元
		private List<NetworkNode> hiddenNodes; //隐藏层的所有神经元
		private List<NetworkNode> outputNodes; //输出层的所有神经元
		
		private double[][] inputHiddenWeight; //输入层到隐藏层的权重
		private double[][] hiddenOutputWeight; //隐藏层到输出层的权重
		...
	}

提供了前向传播的函数，会存储每个神经元的输入和输出，即：

	public void forward(List<Double> input) {
		// input layer
		for (int i = 0; i < input.size(); i++) {
			inputNodes.get(i).setForwardInputValue(input.get(i));
			inputNodes.get(i).setForwardOutputValue(inputNodes.get(i).forwardCompute(input.get(i)));
		}
		// hidden layer
		for (int i = 0; i < hiddenNodeCount; i++) {
			double sum = 0;
			for (int j = 0; j < inputNodeCount; j++) {
				sum += inputHiddenWeight[j][i] * inputNodes.get(j).getForwardOutputValue();
			}
			hiddenNodes.get(i).setForwardInputValue(sum);
			hiddenNodes.get(i).setForwardOutputValue(hiddenNodes.get(i).forwardCompute(sum));
		}
		// output layer
		for (int i = 0; i < outputNodeCount; i++) {
			double sum = 0;
			for (int j = 0; j < hiddenNodeCount; j++) {
				sum += hiddenOutputWeight[j][i] * hiddenNodes.get(j).getForwardOutputValue();
			}
			outputNodes.get(i).setForwardInputValue(sum);
			outputNodes.get(i).setForwardOutputValue(outputNodes.get(i).forwardCompute(sum));
		}
	}

提供了反向传播的函数，存储了反向传播的输入和输出，即：

	public void backward(List<Double> result) {
		// output layer
		for (int i = 0; i < result.size(); i++) {
			double temp = outputNodes.get(i).getForwardOutputValue() - result.get(i);
			outputNodes.get(i).setBackwardInputValue(temp);
			outputNodes.get(i).setBackwardOutputValue(temp * outputNodes.get(i).backwardPropagate(outputNodes.get(i).getForwardInputValue()));
		}
		// hidden layer
		for (int i = 0; i < hiddenNodeCount; i++) {
			double sum = 0;
			for (int j = 0; j < outputNodeCount; j++) {
				sum += outputNodes.get(j).getBackwardOutputValue() * hiddenOutputWeight[i][j];
			}
			hiddenNodes.get(i).setBackwardInputValue(sum);
			hiddenNodes.get(i).setBackwardOutputValue(sum * hiddenNodes.get(i).backwardPropagate(hiddenNodes.get(i).getForwardInputValue()));
		}
	}

提供了更新权重的函数，这里使用了随机梯度下降，即：
	
	public void updateWeight(double learningRate) {
		for (int i = 0; i < inputNodeCount; i++) {
			for (int j = 0; j < hiddenNodeCount; j++) {
				inputHiddenWeight[i][j] -= learningRate * inputNodes.get(i).getForwardOutputValue() * hiddenNodes.get(j).getBackwardOutputValue();
			}
		}
		for (int i = 0; i < hiddenNodeCount; i++) {
			for (int j = 0; j < outputNodeCount; j++) {
				hiddenOutputWeight[i][j] -= learningRate * hiddenNodes.get(i).getForwardOutputValue() * outputNodes.get(j).getBackwardOutputValue();
			}
		}
	}
	
提供了初始化权重的函数，将权重初始化为很小的接近0的值，即：

	public void initializeWeight() {
		for (int i = 0; i < inputNodeCount; i++) {
			for (int j = 0; j < hiddenNodeCount; j++) {
				inputHiddenWeight[i][j] = Math.random() * 0.1;
			}
		}
		for (int i = 0; i < hiddenNodeCount; i++) {
			for (int j = 0; j < outputNodeCount; j++) {
				hiddenOutputWeight[i][j] = Math.random() * 0.1;
			}
		}
	}

这里没有提供训练的代码，可以自己编写训练代码，包括迭代次数、误差阈值等。

#### 激活函数

激活函数作用是在前向传播时计算函数值，反向传播时计算其导数。即：

	public abstract class ActivationFunction {
		public abstract double compute(double i);
		public abstract double derivative(double i);
	}

这里提供 Sigmod 函数的实现。即：

	class Sigmod extends ActivationFunction{
		@Override
		public double compute(double i) {
			return 1.0 / (1+Math.exp(i));
		}
		
		@Override
		public double derivative(double i) {
			return compute(i) * (1 - compute(i));
		}
	}

### 参考资料

1. https://github.com/edvardHua/Articles/blob/master/神经网络中%20BP%20算法的原理与%20Python%20实现源码解析/BP%20算法原理和详细推导流程.pdf
 
2. http://blog.csdn.net/zhongkejingwang/article/details/44514073
